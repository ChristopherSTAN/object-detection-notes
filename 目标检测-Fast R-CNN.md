# Fast RCNN

![img](https://pic3.zhimg.com/v2-deca5ef9583a530eb7249b3b5bbfc04a_b.jpg)

## 在R-CNN的基础上做出改进：

1. Fast RCNN仍然使用selective search选取2000个建议框，但是这里不是将这么多建议框都输入卷积网络中，而是将原始图片输入卷积网络中得到特征图，再使用建议框对特征图提取特征框。这样做的好处是，原来建议框重合部分非常多，卷积重复计算严重，而这里每个位置都只计算了一次卷积，大大减少了计算量
2. 由于建议框大小不一，得到的特征框需要转化为相同大小，这一步是通过ROI池化层来实现的（ROI表示region of interest即目标）
3. Fast RCNN里没有SVM分类器和回归器了，分类和预测框的位置大小都是通过卷积神经网络输出的
4. 为了提高计算速度，网络最后使用SVD代替全连接层

## Fast RCNN的训练步骤：

Fast RCNN采用minibatch的梯度下降，每次使用两张图片的建议框（2 ✖️ 64 = 128）

1. 图片输入到卷积层，得到特征图

2. 根据ground truth标注所有的建议框类别。具体地，对每一个groundtruth边界框，对与其IOU>0.5的bbox标记为此类，而那些在0.1与0.5之间的建议框，标记为背景类别，因此VOC中20个类，处理之后，为21个类

3. 每张图片随机选取64个建议框，其中背景类的建议框占75%，提取出特征框

4. 对于每个输入的建议框，RoI pooling层提取出固定长度的特征向量，输入到图中的FC层中

5. 最后得到两种输出：

   1. 一类输出是softmax，得到21个类的概率向量，表示建议框的类别属性
   2. 另一个是BBox的回归输出，为对应20个类（除了背景）的四个真实值，(x, y, w, h)，其中(x, y)是左上角的坐标。因此，输出为一个20✖️4的矩阵。

6. 从上一点可见，Fast RCNN需要是一种多任务框架，因此需要多种损失函数，每个RoI已经标记为类别真实值 u 和 bbbox回归真实值 v。因此任务中的损失函数为：
   $$
   L(p,u,t_u,v)=L_{cls}(p,u)+\lambda[u\geq1]L_{loc}(t^u,v)
   $$
   

   **注意**：$[u\geq1]$ 的含义：由于在建议框的打标签过程中，我们是将背景框作为$u=0$ ，因此如果是背景，就不计算回归loss了。

   

   其中，分类损失函数为真类的概率的对数损失（log loss）：
   $$
   L_{cks}(p,u) = -log[p_u]
   $$
   而**回归损失**为，对于类 **u** 的真实值
   $$
   t^u = (t^u_x ,t^u_y ,t^u_w ,t^u_h )
   $$
   定义为
   $$
   L_{loc}(t^u, v) = \sum_{i\in \{x,y,w,h\}} smooth_{L_1}(t^u_i-v_i) \\
   smooth_{L_1}(x) = \begin{cases} 0.5 x^2, \quad if \ |x| < 1 \\ |x| -0.5,\quad othersise\end{cases}
   $$

## Fast RCNN的test步骤：

1. 将任意大小的图片输入到CNN网络中，得到Conv特征图；
2. 在该图片中采用selective search算法，提取大约2000个建议框；
3. 根据原图中建议框的位置，按照比例将建议框缩放到特征图上的特征框，**深度与特征图一致**，并在ROI pooling层中将每个特征框池化为 **$H\times W$**（ **VGG-16中是7*7**）的大小；
4. 经过全连接层，特征框们被转化为固定大小的特征向量；
5. 经过全连接层 **由SVD分解实现**，得到softmax分类得分和Bbox回归；
6. 对每一个类，采用nms算法得到输出。



## 细节问题

### 为什么测试的时候可以只进行一次CNN特征提取的操作？

​		我们可以看看R-CNN网络，其先采用selective search来提取2k个建议框（proposals），并对所有建议框进行CNN特征提取，建议框本身就会重叠，那么对所有建议框进行特征提取，将会非常耗费时间和空间。因此只需要对原始输入图片进行特征提取，随后找到建议框**对应的位置**。



### 为什么将每个建议框对应的生成的特征框池化到$H\times W$大小？

1. 像Alex CNN等网络，在提取特征的过程中，对图像的大小没有要求，只是在全连接层的时候才需要大小对齐，因此Fast RCNN可以输入任意大小的图片，并在全连接层前加入RoI池化层，将建议框对应特征图中的特征框池化为$H\times W$的特征向量。
2. **具体实现**：
   1. 首先，将建议框对应于特征图中的大小为$h\times w$的特征框，划分为$H\times W$个sub-windows，那么每个窗口的大小为$\frac{h}{H}\times \frac{w}{W}$；
   2. 然后对每个子窗口采用max-pooling操作，每个子窗口提取出一个最大值，那么特征框大小即为$H\times W$。



### 为什么采用SVD分解实现最后的全连接层？

1. 在当时的目标检测任务中，selective search生成的建议框足足两千个，（**测试**），导致很多时间浪费在全连接层上（**图像分类中，卷积层的时间远大于全连接层**），因此SVD加速非常有必要；

2. 具体实现：

   1. 首先，记全连接层的输入数据为$x$，输出数据为$y$，全连接层参数为$W$，尺寸为$u\times v$，全连接层的计算为：
      $$
      y=Wx
      $$
      计算复杂度为$u\times v$。

   2. 如果将$W$进行SVD分解，并用前$t$个特征值近似替代，即：
      $$
      W=U\sum V^T \approx U(u,1:t)\cdot \sum (1:t,1:t) \cdot V(v,1:t)^T
      $$
      那么此时的前向传播分解为两步：
      $$
      y = Wx=U\cdot(\sum B^T)\cdot x=U\cdot z
      $$
      计算复杂度为$u\times t+v\times t$，如果$t<min(u,v)$，那么计算量将大大减少；

   3. 在具体实现上：相当于把一个全连接层拆分为两个全连接层，第一个不包含bias，第二个则有bias。经过实验，SVD分解在值降低0.3%的mAP的情况下，将速度提升了30%。

      ![这里写图片描述](https://img-blog.csdn.net/20160908165731546)



### 候选框是越多越好吗？

经过作者的实验，在selective search中，提取1k到10k的建议框，随着候选框的增加，mAP先增加后下降。



### 如何处理尺度不变性问题？

尺度不变性，即输入图片在不同尺度下是否能有同样的准确率。

1. 单一尺度：在训练测试阶段，将输入图像固定，期望网络自动学习到具有尺度不变的特征。
2. 多尺度：在训练阶段，随机从图像金字塔（**缩放图像，相当于数据增强**）中采样，再输入进行训练。测试阶段，将图像缩放为金字塔中最接近的尺寸进行训练测试。

而在论文中，作者发现单一尺度比多尺度的mAP仅仅低了1.2%～1.5%，而测试速度快了不少。证明了深度神经网络学习的特征具有尺度不变性的特点。

### 为什么不像R-CNN一样继续采用SVM进行分类

经过实验，softmax的mAP略比SVM高，而且softmax分类不需要额外的中间文件。



​		

